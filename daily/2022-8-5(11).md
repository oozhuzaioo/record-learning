# 今日总结
2022-8-5

## Transformer
[资料](https://juejin.cn/post/6844903953449091080)  
### what is Transformer?  
* Transformer抛弃了以往深度学习任务里面使用到的 CNN 和 RNN， 将Attention思想发挥到极致。  
### 内容  
* 每一个encoder和decoder的内部结构
![image](/images/8-5(transformer-model).png)  

* 整体结构
![image](/images/8-5(transformer-all-model).png)  

* Encoder层结构  
    * 输入数据 -> embedding -> self-attention -> 前馈神经网络 -> 下一个encoder

* Encoder层
    * Positional Encoding
        * 记录单词的顺序

    * Self-Attention
        * 让机器把句子中的某一个单词和句子中其他单词联系起来。
        * Thinking machines 句子举例  
        ![image](/images/8-5(transformer-TM).png)  

    * Multi-Headed Attention
        * self-attention加入了另外一个机制， 被称为“multi-headed” attention
        * 不仅仅只初始化一组Q、K、V的矩阵，而是初始化多组，tranformer是使用了8组，所以最后得到的结果是8个矩阵

    * Feed Forward Neural Network  
        * 把 8 个矩阵降为 1 个的方法

* Decoder层
    * masked mutil-head attetion
        * mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果
    
    * Output层
        * 结尾再添加一个全连接层和softmax层，假如我们的词典是1w个词，那最终softmax会输入1w个词的概率，概率值最大的对应的词就是我们最终的结果

### 动态流程图  
编码阶段：  
![image](/images/8-5(transformer-encoder).gif)  
解码阶段：
![image](/images/8-5(transformer-decoder).gif)  


